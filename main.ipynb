{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - What movie to watch tonight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this homework is to build a search engine over a list of movies that have a dedicated page on Wikipedia. In order to achieve this goal, our work has been divided into different tasks that allowed us to fulfill the intent. Since we based the entire process upon using custom functions (choice due to the will of lightening the code and make it more readable), you won't find much commands, and we suggest to check the py files stored in this repository to better understand every step of which the notebook consists.\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to be taken, as a means to build the search engine, will be scraping the Wikipedia pages and store every information needed to build a dataframe from which our SE picks the useful ones and returns the results of a query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, we need the URLs. To acquire them, we created a function to download, directly from the page indicated by our TAs, the ones needed for the crawling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collector_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_urls = [collector_utils.get_urls(i) for i in range(1,3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon',\n",
       " 2: 'https://en.wikipedia.org/wiki/The_Martyred_Presidents',\n",
       " 3: 'https://en.wikipedia.org/wiki/Terrible_Teddy,_the_Grizzly_King',\n",
       " 4: 'https://en.wikipedia.org/wiki/Jack_and_the_Beanstalk_(1902_film)',\n",
       " 5: 'https://en.wikipedia.org/wiki/Alice_in_Wonderland_(1903_film)',\n",
       " 6: 'https://en.wikipedia.org/wiki/The_Great_Train_Robbery_(1903_film)',\n",
       " 7: 'https://en.wikipedia.org/wiki/The_Suburbanite',\n",
       " 8: 'https://en.wikipedia.org/wiki/The_Little_Train_Robbery',\n",
       " 9: 'https://en.wikipedia.org/wiki/The_Night_Before_Christmas_(1905_film)',\n",
       " 10: 'https://en.wikipedia.org/wiki/Dream_of_a_Rarebit_Fiend_(1906_film)'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(movies_urls[0].items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10001: 'https://en.wikipedia.org/wiki/10_to_Midnight',\n",
       " 10002: 'https://en.wikipedia.org/wiki/All_the_Right_Moves_(film)',\n",
       " 10003: 'https://en.wikipedia.org/wiki/Americana_(film)',\n",
       " 10004: 'https://en.wikipedia.org/wiki/Amityville_3-D',\n",
       " 10005: 'https://en.wikipedia.org/wiki/Anna_to_the_Infinite_Power',\n",
       " 10006: 'https://en.wikipedia.org/wiki/Baby_It%27s_You_(film)',\n",
       " 10007: 'https://en.wikipedia.org/wiki/Bad_Boys_(1983_film)',\n",
       " 10008: 'https://en.wikipedia.org/wiki/Better_Late_Than_Never_(film)',\n",
       " 10009: 'https://en.wikipedia.org/wiki/The_Big_Chill_(film)',\n",
       " 10010: 'https://en.wikipedia.org/wiki/The_Black_Stallion_Returns'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(movies_urls[1].items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we generate two distinct dictionaries containing, for each list, a set of indexd URLs compliant to the original order.\n",
    "Once yielded, we merge them and get the one that is going to be exploited for the latter purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_urls[0].update(movies_urls[1])\n",
    "movies_urls = movies_urls[0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start crawling the pages linked to the URLs. The custom function takes into account the possibility to get bloked by Wikipedia if too many requests are sent. To avoid this risk, we set a 20 minutes pause in case of [Response: 429]. Between any other two successful requests, instead, a random time lying within the range of 1-5 seconds is waited. If any other response code occurs, it just gets skipped and the loop continues (in this domain we find the case of non-existing pages). At the end of each iteration, an html file version of the page is stored in our laptop, within an 'Htmls' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector_utils.scraping(movies_urls, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we start parsing the htmls, that means: extracting from each one a list of specific informations that are going to be stored in a dataframe and used to retrieve the results after each research. In particular, we want to keep the following stuff:\n",
    "<br>\n",
    "**1. Title**\n",
    "<br>\n",
    "**2. Intro**\n",
    "<br>\n",
    "**3. Plot**\n",
    "<br>\n",
    "**4. Film name**\n",
    "<br>\n",
    "**5. Director**\n",
    "<br>\n",
    "**6. Producer**\n",
    "<br>\n",
    "**7. Writer**\n",
    "<br>\n",
    "**8. Starring**\n",
    "<br>\n",
    "**9. Music**\n",
    "<br>\n",
    "**10. Release date**\n",
    "<br>\n",
    "**11. Runtime**\n",
    "<br>\n",
    "**12. Country**\n",
    "<br>\n",
    "**13. Language**\n",
    "<br>\n",
    "**14. Budget**\n",
    "<br>\n",
    "Our final decision was, since not every Wiki page show the same template for the infoboxes dedicated to movies, to just exclude those ones that did not fit the scheme we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parser_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = ['Htmls/' + file for file in os.listdir('Htmls') if file[-4:] == 'html']\n",
    "ls.sort(key = lambda x: int(''.join(filter(str.isdigit, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = list()\n",
    "fails = 0\n",
    "\n",
    "try:\n",
    "    os.makedirs('Tsv')\n",
    "except:\n",
    "    _ = None\n",
    "\n",
    "for i in tqdm(range(len(ls))):\n",
    "    path_file = ls[i]\n",
    "    movie_info = parser_utils.info_extractor(path_file)\n",
    "    if movie_info != False:\n",
    "        movie_data.append(movie_info)\n",
    "        pd.DataFrame(list(movie_info.values())).to_csv(r'Tsv\\Article_' + str(i + 1) + '.tsv', sep='\\t', header = False, index = False)\n",
    "    else:\n",
    "        fails += 1\n",
    "        #print(ls[i])\n",
    "\n",
    "print(str(fails) + ' files are not Wikipedia movie pages')\n",
    "movies_info_df = pd.DataFrame(movie_data)\n",
    "movies_info_df['Doc_ID'] = movies_info_df.index\n",
    "movies_info_df.to_csv('movie_data.csv')\n",
    "movies_info_df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for each iteration a tsv file is created and then appended to a main dataframe which we will later use for our intentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the preparatory phase has been brought to completion, we can start developing the main object of this homework: the search engine itself.\n",
    "<br>\n",
    "What we need to do, primarily, is running a preprocessing procedure. This will remove punctuation, stopwords and stem each document. We do not show it here, since our custom **text_cleaner** function is stored inside the **utils.py** file and is later used within the **index_utils.py** one, in which we decided to create a specific class of elements and methods to ease the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, we narrow our interest on the intro and plot of each document. It means that the first Search Engine will evaluate queries with respect to the aforementioned information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, the structure of our code was thought as finalized to get the py files recommended in the instructions, so it is not possible to split everything and show it in the notebook. Anyway, all that is needed to create the index is inside the **index_utils.py** library. What happens, basically, since we defined a class, is that, after initializing an _se_ object, we run a custom method (belonging to the class) called **create_engine**, and this one directly executes a series of commands that generate the index and the vocabulary for a specific search engine (which number will be taken as input, as well as the dataframe above which the procedures must be run).\n",
    "<br>\n",
    "The inverted index yielded will have the following shape:\n",
    "<br>\n",
    "$\\{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "\\\\ \\ \\ term_id_2:[document_1, document_3, document_5, document_6],\\ ...\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MOSES\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MOSES\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import index_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataframe\n",
    "movies_df = pd.read_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the object and running the method\n",
    "se = index_utils.search_engine()\n",
    "se.create_engine(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the resulting output\n",
    "pickle.dump(se, open(\"se.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now execute a query over the built search engine. The output will be the documents containing all the words searched, of which we just show the title, the intro and the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disney movie 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MOSES\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MOSES\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "se = pickle.load(open( \"se.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12875</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>\\n\\nCars 3 is a 2017 American 3D computer-anim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11634</th>\n",
       "      <td>Frozen</td>\n",
       "      <td>\\n\\nFrozen is a 2013 American 3D computer-anim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Thumbelina (also known as Hans Christian Ander...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11030</th>\n",
       "      <td>The Avengers</td>\n",
       "      <td>\\n\\nMarvel's The Avengers[6] (classified under...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>\\nThe Chronicles of Narnia: Prince Caspian is ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Title  \\\n",
       "12875                                    Cars 3   \n",
       "11634                                    Frozen   \n",
       "4211                                 Thumbelina   \n",
       "11030                              The Avengers   \n",
       "9658   The Chronicles of Narnia: Prince Caspian   \n",
       "\n",
       "                                                   Intro  \\\n",
       "12875  \\n\\nCars 3 is a 2017 American 3D computer-anim...   \n",
       "11634  \\n\\nFrozen is a 2013 American 3D computer-anim...   \n",
       "4211   Thumbelina (also known as Hans Christian Ander...   \n",
       "11030  \\n\\nMarvel's The Avengers[6] (classified under...   \n",
       "9658   \\nThe Chronicles of Narnia: Prince Caspian is ...   \n",
       "\n",
       "                                                     Url  \n",
       "12875               https://en.wikipedia.org/wiki/Cars_3  \n",
       "11634   https://en.wikipedia.org/wiki/Frozen_(2013_film)  \n",
       "4211   https://en.wikipedia.org/wiki/Thumbelina_(1994...  \n",
       "11030  https://en.wikipedia.org/wiki/The_Avengers_(20...  \n",
       "9658   https://en.wikipedia.org/wiki/The_Chronicles_o...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.query(search_engine = 1, q = query, dataframe = movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second search engine, given a query, we are assigned the task get the top-k documents related to the latter. \n",
    "In particular, we have to:\n",
    "- find all the documents that contains all the words in the query.\n",
    "- sort them by their similarity with the query\n",
    "- return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Inverted index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same old story: within the module **create_engine**, by means of an if condition, we specify the search engine number, so, when the search engine to produce is the second one, an inverted index based upon a _Tf-Idf_ score (computed real-time) gets created. The resulting shape will be the following: \n",
    "$ \\{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}),\\  ...],\n",
    "\\\\\n",
    "\\ \\ term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}),\\\\ ...],\\ ...\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the object and running the method. The second search engine is already trained via create_engnie() method.\n",
    "se = index_utils.search_engine()\n",
    "se.create_engine(movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the module to create a search engine with its corresponding index and vocabulary, our class has been furthermore provided of another one, called **query** (already used before), which directly computes the _Cosine similarity_ in case of search engines 2 & 3. In this way, when we browse it, our final output will be a list of documents, ranked by their Cosine similarity with respect to the query entered in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Thumbelina (also known as Hans Christian Ander...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "      <td>0.999152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11030</th>\n",
       "      <td>The Avengers</td>\n",
       "      <td>\\n\\nMarvel's The Avengers[6] (classified under...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "      <td>0.999003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12875</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>\\n\\nCars 3 is a 2017 American 3D computer-anim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "      <td>0.996418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>\\nThe Chronicles of Narnia: Prince Caspian is ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "      <td>0.987701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11634</th>\n",
       "      <td>Frozen</td>\n",
       "      <td>\\n\\nFrozen is a 2013 American 3D computer-anim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "      <td>0.984607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Title  \\\n",
       "4211                                 Thumbelina   \n",
       "11030                              The Avengers   \n",
       "12875                                    Cars 3   \n",
       "9658   The Chronicles of Narnia: Prince Caspian   \n",
       "11634                                    Frozen   \n",
       "\n",
       "                                                   Intro  \\\n",
       "4211   Thumbelina (also known as Hans Christian Ander...   \n",
       "11030  \\n\\nMarvel's The Avengers[6] (classified under...   \n",
       "12875  \\n\\nCars 3 is a 2017 American 3D computer-anim...   \n",
       "9658   \\nThe Chronicles of Narnia: Prince Caspian is ...   \n",
       "11634  \\n\\nFrozen is a 2013 American 3D computer-anim...   \n",
       "\n",
       "                                                     Url     Score  \n",
       "4211   https://en.wikipedia.org/wiki/Thumbelina_(1994...  0.999152  \n",
       "11030  https://en.wikipedia.org/wiki/The_Avengers_(20...  0.999003  \n",
       "12875               https://en.wikipedia.org/wiki/Cars_3  0.996418  \n",
       "9658   https://en.wikipedia.org/wiki/The_Chronicles_o...  0.987701  \n",
       "11634   https://en.wikipedia.org/wiki/Frozen_(2013_film)  0.984607  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.query(search_engine = 2, q = query, dataframe = movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this task we were asked to define a new score by which the results for a query could be ranked in a more efficacious way. Dealing with movies-centered data, it has been somewhat hard to come out with an idea. Our way to manage the job has consisted in suggesting the user a series of additional questions to guide him through the choice of more specific information. In particular, after computing the frequency by which a string appears inside different columns, the search engine recommends the user how to rank the data that most properly match its request. In this way a different weight (score = score * 1.02) will be assigned to the column to which we have been addressed.\n",
    "<br>\n",
    "Practical example: the user types \"Brad Pitt\", after retrieving info about movies that contains the specified string, the search engine will generate some questions like \"Do you mean Directed by: brad?\", \"Do you mean Directed by: pitt?\", \"Do you mean Starring: brad?\", \"Do you mean Starring: pitt?\" and \"Do you mean Starring: brad pitt?\". Of course, since we are dealing with conjunctive queries, the highest importance will be assigned to the results contemplating the whole string taken as input (in this case \"brad pitt\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frequency  length    word           col_name  Score\n",
      "0          4       6  disney     Distributed by    4.6\n",
      "1          3       6  disney  Productioncompany    3.9\n",
      "do you mean Distributed by: disney? Y/n:y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11030</th>\n",
       "      <td>The Avengers</td>\n",
       "      <td>\\n\\nMarvel's The Avengers[6] (classified under...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Avengers_(20...</td>\n",
       "      <td>1.018983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12875</th>\n",
       "      <td>Cars 3</td>\n",
       "      <td>\\n\\nCars 3 is a 2017 American 3D computer-anim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cars_3</td>\n",
       "      <td>1.016346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>The Chronicles of Narnia: Prince Caspian</td>\n",
       "      <td>\\nThe Chronicles of Narnia: Prince Caspian is ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Chronicles_o...</td>\n",
       "      <td>1.007455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11634</th>\n",
       "      <td>Frozen</td>\n",
       "      <td>\\n\\nFrozen is a 2013 American 3D computer-anim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Frozen_(2013_film)</td>\n",
       "      <td>1.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>Thumbelina</td>\n",
       "      <td>Thumbelina (also known as Hans Christian Ander...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thumbelina_(1994...</td>\n",
       "      <td>0.999152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Title  \\\n",
       "11030                              The Avengers   \n",
       "12875                                    Cars 3   \n",
       "9658   The Chronicles of Narnia: Prince Caspian   \n",
       "11634                                    Frozen   \n",
       "4211                                 Thumbelina   \n",
       "\n",
       "                                                   Intro  \\\n",
       "11030  \\n\\nMarvel's The Avengers[6] (classified under...   \n",
       "12875  \\n\\nCars 3 is a 2017 American 3D computer-anim...   \n",
       "9658   \\nThe Chronicles of Narnia: Prince Caspian is ...   \n",
       "11634  \\n\\nFrozen is a 2013 American 3D computer-anim...   \n",
       "4211   Thumbelina (also known as Hans Christian Ander...   \n",
       "\n",
       "                                                     Url     Score  \n",
       "11030  https://en.wikipedia.org/wiki/The_Avengers_(20...  1.018983  \n",
       "12875               https://en.wikipedia.org/wiki/Cars_3  1.016346  \n",
       "9658   https://en.wikipedia.org/wiki/The_Chronicles_o...  1.007455  \n",
       "11634   https://en.wikipedia.org/wiki/Frozen_(2013_film)  1.004300  \n",
       "4211   https://en.wikipedia.org/wiki/Thumbelina_(1994...  0.999152  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'disney movie 2019'\n",
    "se.query(search_engine = 3, q = query, dataframe = movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frequency  length       word     col_name  Score\n",
      "0         31       9  brad pitt     Starring   24.4\n",
      "1         32       4       brad     Starring   23.6\n",
      "2         32       4       pitt     Starring   23.6\n",
      "3          6       9  brad pitt  Produced by    6.9\n",
      "4          6       4       brad  Produced by    5.4\n",
      "5          6       4       pitt  Produced by    5.4\n",
      "6          1       9  pitt brad     Starring    3.4\n",
      "do you mean Starring: brad pitt? Y/n:y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10778</th>\n",
       "      <td>Happy Feet Two</td>\n",
       "      <td>\\nHappy Feet Two (or Happy Feet 2) is a 2011 c...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Happy_Feet_Two</td>\n",
       "      <td>1.019958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>Troy</td>\n",
       "      <td>Troy is a 2004 epic historical war drama film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Troy_(film)</td>\n",
       "      <td>1.019953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>Legends of the Fall</td>\n",
       "      <td>Legends of the Fall is a 1994 American epic hi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Legends_of_the_Fall</td>\n",
       "      <td>1.019940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>Snatch</td>\n",
       "      <td>\\nSnatch (stylized as snatch.) is a 2000 Briti...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Snatch_(film)</td>\n",
       "      <td>1.019939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9197</th>\n",
       "      <td>The Assassination of Jesse James by the Coward...</td>\n",
       "      <td>The Assassination of Jesse James by the Coward...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Assassinatio...</td>\n",
       "      <td>1.019938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7107</th>\n",
       "      <td>Spy Game</td>\n",
       "      <td>Spy Game is a 2001 American spy film directed ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Spy_Game</td>\n",
       "      <td>1.019936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12857</th>\n",
       "      <td>War Machine</td>\n",
       "      <td>War Machine is a 2017 American satirical war f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/War_Machine_(film)</td>\n",
       "      <td>1.019928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8107</th>\n",
       "      <td>Ocean's Twelve</td>\n",
       "      <td>Ocean's Twelve is a 2004 American heist film d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ocean%27s_Twelve</td>\n",
       "      <td>1.019917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>Cool World</td>\n",
       "      <td>\\nCool World is a 1992 American live-action/an...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cool_World</td>\n",
       "      <td>1.019917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>Seven</td>\n",
       "      <td>\\nSeven (stylized as SE7EN) is a 1995 American...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Seven_(1995_film)</td>\n",
       "      <td>1.019908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>A River Runs Through It</td>\n",
       "      <td>A River Runs Through It is a 1992 American dra...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A_River_Runs_Thr...</td>\n",
       "      <td>1.019905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10950</th>\n",
       "      <td>The Tree of Life</td>\n",
       "      <td>\\nThe Tree of Life is a 2011 American experime...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Tree_of_Life...</td>\n",
       "      <td>1.019895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>Sleepers</td>\n",
       "      <td>Sleepers is a 1996 American legal crime drama ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sleepers</td>\n",
       "      <td>1.019893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>True Romance</td>\n",
       "      <td>True Romance is a 1993 American romantic crime...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/True_Romance</td>\n",
       "      <td>1.019854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>The Mexican</td>\n",
       "      <td>The Mexican is a 2001 American comedy film dir...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Mexican</td>\n",
       "      <td>1.019854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>Cutting Class</td>\n",
       "      <td>Cutting Class is a 1989 American dark comedy s...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cutting_Class</td>\n",
       "      <td>1.019820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>Across the Tracks</td>\n",
       "      <td>Across the Tracks is a 1991 American drama fil...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Across_the_Tracks</td>\n",
       "      <td>1.019767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>Seven Years in Tibet</td>\n",
       "      <td>\\nSeven Years in Tibet is a 1997 American biog...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Seven_Years_in_T...</td>\n",
       "      <td>1.017680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11986</th>\n",
       "      <td>World War Z</td>\n",
       "      <td>\\nWorld War Z is a 2013 American apocalyptic a...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/World_War_Z_(film)</td>\n",
       "      <td>1.017412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7820</th>\n",
       "      <td>Sinbad:Legend of the Seven Seas</td>\n",
       "      <td>Sinbad: Legend of the Seven Seas is a 2003 Ame...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sinbad:_Legend_o...</td>\n",
       "      <td>1.017252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>Interview with the Vampire</td>\n",
       "      <td>\\nInterview with the Vampire is a 1994 America...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Interview_with_t...</td>\n",
       "      <td>1.017080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>Inglourious Basterds</td>\n",
       "      <td>\\nInglourious Basterds is a 2009 revisionist w...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inglourious_Bast...</td>\n",
       "      <td>1.016968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9668</th>\n",
       "      <td>The Curious Case of Benjamin Button</td>\n",
       "      <td>\\nThe Curious Case of Benjamin Button is a 200...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Curious_Case...</td>\n",
       "      <td>1.016222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9649</th>\n",
       "      <td>Burn After Reading</td>\n",
       "      <td>\\nBurn After Reading is a 2008 black comedy fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Burn_After_Reading</td>\n",
       "      <td>1.016168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>Ocean's Eleven</td>\n",
       "      <td>\\nOcean's Eleven is a 2001 American heist film...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ocean%27s_Eleven</td>\n",
       "      <td>1.015893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6235</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>\\nFight Club is a 1999 film directed by David ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fight_Club</td>\n",
       "      <td>1.015838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>Moneyball</td>\n",
       "      <td>\\nMoneyball is a 2011 American sports film dir...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Moneyball_(film)</td>\n",
       "      <td>1.015611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665</th>\n",
       "      <td>12 Monkeys</td>\n",
       "      <td>\\n\\n12 Monkeys, also known as Twelve Monkeys, ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/12_Monkeys</td>\n",
       "      <td>1.015131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>Mr. &amp; Mrs. Smith</td>\n",
       "      <td>\\nMr. &amp; Mrs. Smith is a 2005 American action c...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mr._%26_Mrs._Smi...</td>\n",
       "      <td>1.014318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>Meet Joe Black</td>\n",
       "      <td>Meet Joe Black is a 1998 American romantic fan...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Meet_Joe_Black</td>\n",
       "      <td>1.013855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>Kalifornia</td>\n",
       "      <td>Kalifornia is a 1993 American road thriller fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kalifornia</td>\n",
       "      <td>1.007962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>Abby Singer</td>\n",
       "      <td>\\nAbby Singer is a 2003 dark comedy film tale ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abby_Singer_(film)</td>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9796</th>\n",
       "      <td>Meet the Spartans</td>\n",
       "      <td>\\nMeet the Spartans is a 2008 American parody ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Meet_the_Spartans</td>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10476</th>\n",
       "      <td>Kick-Ass</td>\n",
       "      <td>\\nKick-Ass is a 2010 superhero black comedy fi...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kick-Ass_(film)</td>\n",
       "      <td>0.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3126</th>\n",
       "      <td>Thelma &amp; Louise</td>\n",
       "      <td>\\nThelma &amp; Louise is a 1991 American female bu...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Thelma_%26_Louise</td>\n",
       "      <td>0.999935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>Bully</td>\n",
       "      <td>Bully is a 2001 American crime film directed b...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bully_(2001_film)</td>\n",
       "      <td>0.999928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "10778                                     Happy Feet Two   \n",
       "8191                                                Troy   \n",
       "4032                                 Legends of the Fall   \n",
       "6726                                              Snatch   \n",
       "9197   The Assassination of Jesse James by the Coward...   \n",
       "7107                                            Spy Game   \n",
       "12857                                        War Machine   \n",
       "8107                                      Ocean's Twelve   \n",
       "3209                                          Cool World   \n",
       "4610                                               Seven   \n",
       "3398                             A River Runs Through It   \n",
       "10950                                   The Tree of Life   \n",
       "5119                                            Sleepers   \n",
       "3788                                        True Romance   \n",
       "7010                                         The Mexican   \n",
       "2062                                       Cutting Class   \n",
       "2815                                   Across the Tracks   \n",
       "5625                                Seven Years in Tibet   \n",
       "11986                                        World War Z   \n",
       "7820                     Sinbad:Legend of the Seven Seas   \n",
       "4005                          Interview with the Vampire   \n",
       "10124                               Inglourious Basterds   \n",
       "9668                 The Curious Case of Benjamin Button   \n",
       "9649                                  Burn After Reading   \n",
       "7037                                      Ocean's Eleven   \n",
       "6235                                          Fight Club   \n",
       "10855                                          Moneyball   \n",
       "4665                                          12 Monkeys   \n",
       "8463                                    Mr. & Mrs. Smith   \n",
       "5926                                      Meet Joe Black   \n",
       "3649                                          Kalifornia   \n",
       "7525                                         Abby Singer   \n",
       "9796                                   Meet the Spartans   \n",
       "10476                                           Kick-Ass   \n",
       "3126                                     Thelma & Louise   \n",
       "6851                                               Bully   \n",
       "\n",
       "                                                   Intro  \\\n",
       "10778  \\nHappy Feet Two (or Happy Feet 2) is a 2011 c...   \n",
       "8191   Troy is a 2004 epic historical war drama film ...   \n",
       "4032   Legends of the Fall is a 1994 American epic hi...   \n",
       "6726   \\nSnatch (stylized as snatch.) is a 2000 Briti...   \n",
       "9197   The Assassination of Jesse James by the Coward...   \n",
       "7107   Spy Game is a 2001 American spy film directed ...   \n",
       "12857  War Machine is a 2017 American satirical war f...   \n",
       "8107   Ocean's Twelve is a 2004 American heist film d...   \n",
       "3209   \\nCool World is a 1992 American live-action/an...   \n",
       "4610   \\nSeven (stylized as SE7EN) is a 1995 American...   \n",
       "3398   A River Runs Through It is a 1992 American dra...   \n",
       "10950  \\nThe Tree of Life is a 2011 American experime...   \n",
       "5119   Sleepers is a 1996 American legal crime drama ...   \n",
       "3788   True Romance is a 1993 American romantic crime...   \n",
       "7010   The Mexican is a 2001 American comedy film dir...   \n",
       "2062   Cutting Class is a 1989 American dark comedy s...   \n",
       "2815   Across the Tracks is a 1991 American drama fil...   \n",
       "5625   \\nSeven Years in Tibet is a 1997 American biog...   \n",
       "11986  \\nWorld War Z is a 2013 American apocalyptic a...   \n",
       "7820   Sinbad: Legend of the Seven Seas is a 2003 Ame...   \n",
       "4005   \\nInterview with the Vampire is a 1994 America...   \n",
       "10124  \\nInglourious Basterds is a 2009 revisionist w...   \n",
       "9668   \\nThe Curious Case of Benjamin Button is a 200...   \n",
       "9649   \\nBurn After Reading is a 2008 black comedy fi...   \n",
       "7037   \\nOcean's Eleven is a 2001 American heist film...   \n",
       "6235   \\nFight Club is a 1999 film directed by David ...   \n",
       "10855  \\nMoneyball is a 2011 American sports film dir...   \n",
       "4665   \\n\\n12 Monkeys, also known as Twelve Monkeys, ...   \n",
       "8463   \\nMr. & Mrs. Smith is a 2005 American action c...   \n",
       "5926   Meet Joe Black is a 1998 American romantic fan...   \n",
       "3649   Kalifornia is a 1993 American road thriller fi...   \n",
       "7525   \\nAbby Singer is a 2003 dark comedy film tale ...   \n",
       "9796   \\nMeet the Spartans is a 2008 American parody ...   \n",
       "10476  \\nKick-Ass is a 2010 superhero black comedy fi...   \n",
       "3126   \\nThelma & Louise is a 1991 American female bu...   \n",
       "6851   Bully is a 2001 American crime film directed b...   \n",
       "\n",
       "                                                     Url     Score  \n",
       "10778       https://en.wikipedia.org/wiki/Happy_Feet_Two  1.019958  \n",
       "8191           https://en.wikipedia.org/wiki/Troy_(film)  1.019953  \n",
       "4032   https://en.wikipedia.org/wiki/Legends_of_the_Fall  1.019940  \n",
       "6726         https://en.wikipedia.org/wiki/Snatch_(film)  1.019939  \n",
       "9197   https://en.wikipedia.org/wiki/The_Assassinatio...  1.019938  \n",
       "7107              https://en.wikipedia.org/wiki/Spy_Game  1.019936  \n",
       "12857   https://en.wikipedia.org/wiki/War_Machine_(film)  1.019928  \n",
       "8107      https://en.wikipedia.org/wiki/Ocean%27s_Twelve  1.019917  \n",
       "3209            https://en.wikipedia.org/wiki/Cool_World  1.019917  \n",
       "4610     https://en.wikipedia.org/wiki/Seven_(1995_film)  1.019908  \n",
       "3398   https://en.wikipedia.org/wiki/A_River_Runs_Thr...  1.019905  \n",
       "10950  https://en.wikipedia.org/wiki/The_Tree_of_Life...  1.019895  \n",
       "5119              https://en.wikipedia.org/wiki/Sleepers  1.019893  \n",
       "3788          https://en.wikipedia.org/wiki/True_Romance  1.019854  \n",
       "7010           https://en.wikipedia.org/wiki/The_Mexican  1.019854  \n",
       "2062         https://en.wikipedia.org/wiki/Cutting_Class  1.019820  \n",
       "2815     https://en.wikipedia.org/wiki/Across_the_Tracks  1.019767  \n",
       "5625   https://en.wikipedia.org/wiki/Seven_Years_in_T...  1.017680  \n",
       "11986   https://en.wikipedia.org/wiki/World_War_Z_(film)  1.017412  \n",
       "7820   https://en.wikipedia.org/wiki/Sinbad:_Legend_o...  1.017252  \n",
       "4005   https://en.wikipedia.org/wiki/Interview_with_t...  1.017080  \n",
       "10124  https://en.wikipedia.org/wiki/Inglourious_Bast...  1.016968  \n",
       "9668   https://en.wikipedia.org/wiki/The_Curious_Case...  1.016222  \n",
       "9649    https://en.wikipedia.org/wiki/Burn_After_Reading  1.016168  \n",
       "7037      https://en.wikipedia.org/wiki/Ocean%27s_Eleven  1.015893  \n",
       "6235            https://en.wikipedia.org/wiki/Fight_Club  1.015838  \n",
       "10855     https://en.wikipedia.org/wiki/Moneyball_(film)  1.015611  \n",
       "4665            https://en.wikipedia.org/wiki/12_Monkeys  1.015131  \n",
       "8463   https://en.wikipedia.org/wiki/Mr._%26_Mrs._Smi...  1.014318  \n",
       "5926        https://en.wikipedia.org/wiki/Meet_Joe_Black  1.013855  \n",
       "3649            https://en.wikipedia.org/wiki/Kalifornia  1.007962  \n",
       "7525    https://en.wikipedia.org/wiki/Abby_Singer_(film)  0.999939  \n",
       "9796     https://en.wikipedia.org/wiki/Meet_the_Spartans  0.999939  \n",
       "10476      https://en.wikipedia.org/wiki/Kick-Ass_(film)  0.999939  \n",
       "3126     https://en.wikipedia.org/wiki/Thelma_%26_Louise  0.999935  \n",
       "6851     https://en.wikipedia.org/wiki/Bully_(2001_film)  0.999928  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Brad Pitt'\n",
    "se.query(search_engine = 3, q = query, dataframe = movies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this problem we made multiple attempts, below we report all of them and explain the reasons behind every approach. Our first endeavour is actually very naive and to carry it out we decided to exploit itertools library. Our idea was: starting from the full sequence, if we set up a loop that decreases by one at each iteration,  we can compute all the combinations without replacement, and check if the resulting ones are equal to their reverse. If at least one of them satisfies this condition, the loop stops and we print the length of that which will be the longest palindromic sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First algorithm implementation. Please insert a string: dataminingsapienza2019902\n",
      "It would take too much time.\n",
      "There is no palindromic sequence or the elapsed time was igher than 10 seconds.\n",
      "The algorithm took approximately 10.85 seconds.\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "sequence = input('First algorithm implementation. Please insert a string: ')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "count = set()\n",
    "done = False\n",
    "for i in range(len(sequence), 1, -1):\n",
    "    if (time.time() - start_time) > 10:\n",
    "        break\n",
    "    subsequences = list(combinations(sequence, i))\n",
    "    for k in range(len(subsequences)):\n",
    "        if subsequences[k] == subsequences[k][::-1]:\n",
    "            count.add(len(subsequences[k]))\n",
    "            done = True\n",
    "            break\n",
    "    if done: break\n",
    "\n",
    "try:\n",
    "    print('The longest palindromic subsequence has length: ' + str(max(count)))\n",
    "except ValueError:\n",
    "    print('There is no palindromic sequence or the elapsed time was higher than 10 seconds.')\n",
    "    \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print('The algorithm took approximately ' + str(round(elapsed_time, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous algorithm works, but it's obviously not optimal, because the time taken to run it increases as the sequence length gets longer. Hence we tried to propose an alternative based upon recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second algorithm implementation. Please insert a string: dataminingsapienza20199098\n",
      "The longest palindromic subsequence has length:  7\n",
      "The algorithm took approximately 17.03 seconds.\n"
     ]
    }
   ],
   "source": [
    "def palindrome(sequence, i, j): \n",
    "    if (i == j): \n",
    "        return 1\n",
    "    if (sequence[i] == sequence[j] and i + 1 == j): \n",
    "        return 2\n",
    "    if (sequence[i] == sequence[j]): \n",
    "        return palindrome(sequence, i + 1, j - 1) + 2\n",
    "    return max(palindrome(sequence, i, j - 1), palindrome(sequence, i + 1, j)) \n",
    "\n",
    "sequence = input('Second algorithm implementation. Please insert a string: ')\n",
    "   \n",
    "start_time = time.time()\n",
    "\n",
    "length = len(sequence) \n",
    "\n",
    "if int(length) < 27:\n",
    "    print(\"The longest palindromic subsequence has length: \", palindrome(sequence, 0, length - 1)) \n",
    "else:\n",
    "    print('It would take too much time.')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print('The algorithm took approximately ' + str(round(elapsed_time, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in this case, although an improvement in terms of space is obtained (and that is not trivial), the running time continues to be very long. Our last try is based upon Dynamic Programming, and we wrote the code by studying the algorithm derived from this approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataminingsapienzadataminingsapienza\n",
      "The longest palindromic subsequence has length: 15\n",
      "The algorithm took approximately 0.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "sequence = input('Third algorithm implementation. Please insert a string: ') \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "table = [[1 for x in range(len(sequence))] for x in range(len(sequence))] \n",
    "\n",
    "for i in range(len(sequence)): \n",
    "    table[i][i] = 1\n",
    "\n",
    "for substring_length in range(2, len(sequence) + 1): \n",
    "    for i in range(len(sequence)-substring_length + 1): \n",
    "        j = i + substring_length-1\n",
    "        if sequence[i] == sequence[j] and substring_length == 2: \n",
    "            table[i][j] = 2\n",
    "        elif sequence[i] == sequence[j]: \n",
    "            table[i][j] = table[i + 1][j-1] + 2\n",
    "        else: \n",
    "            table[i][j] = max(table[i][j-1], table[i + 1][j])\n",
    "\n",
    "print(\"The longest palindromic subsequence has length: \" + str(table[0][len(sequence)-1])) \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print('The algorithm took approximately ' + str(round(elapsed_time, 2)) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the running time experience a substantial improvement and even with very long strings the result it's computed quickly and efficiently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
